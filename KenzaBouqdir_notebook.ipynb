{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kenza Bouqdir - Assignment 4 - Movie Recommender System using SVD (with User-Mean Centering)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements a movie recommendation system using Singular Value Decomposition (SVD) with user-mean centering. The system is built using the MovieLens dataset and optimized for performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import seaborn as sns\n",
    "import gc  # Garbage collector for memory management\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "    1- Data Loading and Preparation\n",
    "    2 -Data Exploration\n",
    "    3- Train-Test Split\n",
    "    4- Mean-Centering\n",
    "    5- Sparse Matrix Creation\n",
    "    6- SVD Application\n",
    "    7- Model Selection\n",
    "    8- Recommendation Utilities\n",
    "    9- Sample Recommendations\n",
    "    10- Similar Movies\n",
    "    11- Latent Space Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Loading and Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code begins by loading the MovieLens dataset with memory optimization techniques. It includes:\n",
    "\n",
    "- Memory usage reduction by optimizing data types\n",
    "- Fallback to sampling if memory issues arise\n",
    "- Filtering out users with very few ratings to improve model quality\n",
    "- Filtering out movies with very few ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MOVIE RECOMMENDER SYSTEM USING SVD - CSC5356 DATA ENGINEERING (with USER-MEAN CENTERING)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MOVIE RECOMMENDER SYSTEM USING SVD - CSC5356 DATA ENGINEERING (with USER-MEAN CENTERING)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer to measure performance\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. LOADING AND PREPARING DATASET...\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and prepare the MovieLens dataset with optimization\n",
    "print(\"\\n1. LOADING AND PREPARING DATASET...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to reduce memory usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a progress bar\n",
    "def progress_bar(current, total, bar_length=50):\n",
    "    fraction = current / total\n",
    "    arrow = int(fraction * bar_length) * '█'\n",
    "    padding = (bar_length - len(arrow)) * '░'\n",
    "    print(f'\\r[{arrow}{padding}] {int(fraction * 100)}%', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load full dataset...\n",
      "Memory usage of dataframe is 762.94 MB\n",
      "Memory usage after optimization is: 381.47 MB\n",
      "Decreased by 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Try to load the full dataset, but fall back to sampling if memory issues arise\n",
    "try:\n",
    "    print(\"Attempting to load full dataset...\")\n",
    "    ratings = pd.read_csv('ml-25m/ml-25m/ratings.csv')\n",
    "    use_sample = False\n",
    "except (MemoryError, FileNotFoundError) as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Falling back to sampled dataset due to memory constraints or file not found\")\n",
    "    try:\n",
    "        ratings = pd.read_csv('ratings.csv')\n",
    "    except FileNotFoundError:\n",
    "        ratings = pd.read_csv('ml-25m/ml-25m/ratings.csv')\n",
    "    use_sample = True\n",
    "    sample_size = 1000000  # Increased sample size for better representation\n",
    "\n",
    "if use_sample:\n",
    "    print(f\"Using a sample of {sample_size} ratings due to memory constraints\")\n",
    "    # Stratified sampling by userId to maintain user distribution\n",
    "    user_counts = ratings['userId'].value_counts()\n",
    "    users_to_include = user_counts[user_counts >= 20].index.tolist()\n",
    "    \n",
    "    if len(users_to_include) > 0:\n",
    "        sampled_ratings = ratings[ratings['userId'].isin(users_to_include)]\n",
    "        if len(sampled_ratings) > sample_size:\n",
    "            ratings = sampled_ratings.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            ratings = ratings.sample(n=min(sample_size, len(ratings)), random_state=42)\n",
    "    else:\n",
    "        ratings = ratings.sample(n=min(sample_size, len(ratings)), random_state=42)\n",
    "\n",
    "ratings = reduce_mem_usage(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.43 MB\n",
      "Memory usage after optimization is: 1.19 MB\n",
      "Decreased by 16.7%\n"
     ]
    }
   ],
   "source": [
    "# Load movies data with proper error handling\n",
    "try:\n",
    "    movies = pd.read_csv('ml-25m/ml-25m/movies.csv')\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        movies = pd.read_csv('movies.csv')\n",
    "    except FileNotFoundError:\n",
    "        movies = pd.read_csv('ml-25m/ml-25m/movies.csv')\n",
    "\n",
    "movies = reduce_mem_usage(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with 24974531 ratings from 162541 users\n",
      "Dataset spans 41116 movies\n"
     ]
    }
   ],
   "source": [
    "# Add a timestamp conversion for analysis\n",
    "ratings['date'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "ratings['year'] = ratings['date'].dt.year\n",
    "ratings['month'] = ratings['date'].dt.month\n",
    "\n",
    "# Filter out users with very few ratings to improve model quality\n",
    "min_ratings_per_user = 5\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_ratings_per_user].index\n",
    "ratings = ratings[ratings['userId'].isin(valid_users)]\n",
    "\n",
    "# Filter out movies with very few ratings\n",
    "min_ratings_per_movie = 3\n",
    "movie_counts = ratings['movieId'].value_counts()\n",
    "valid_movies = movie_counts[movie_counts >= min_ratings_per_movie].index\n",
    "ratings = ratings[ratings['movieId'].isin(valid_movies)]\n",
    "\n",
    "print(f\"\\nWorking with {len(ratings)} ratings from {len(ratings['userId'].unique())} users\")\n",
    "print(f\"Dataset spans {len(ratings['movieId'].unique())} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section analyzes the dataset structure and characteristics:\n",
    "\n",
    "- Summary statistics for ratings\n",
    "- Distribution of movie ratings visualization\n",
    "- Analysis of movie genres\n",
    "- Rating trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. EXPLORING DATA STRUCTURE...\n",
      "Summary statistics for ratings:\n",
      "count    2.497453e+07\n",
      "mean     3.534428e+00\n",
      "std      1.060538e+00\n",
      "min      5.000000e-01\n",
      "25%      3.000000e+00\n",
      "50%      3.500000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. EXPLORING DATA STRUCTURE...\")\n",
    "# Generate summary statistics\n",
    "print(\"Summary statistics for ratings:\")\n",
    "rating_stats = ratings['rating'].describe()\n",
    "print(rating_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ratings['rating'], bins=10, kde=True)\n",
    "plt.title('Distribution of Movie Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('rating_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze genres\n",
    "genres_list = []\n",
    "for genres_str in movies['genres'].str.split('|'):\n",
    "    genres_list.extend(genres_str)\n",
    "\n",
    "unique_genres = set(genres_list)\n",
    "genre_counts = {genre: genres_list.count(genre) for genre in unique_genres}\n",
    "\n",
    "# Plot top genres\n",
    "top_genres = dict(sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(top_genres.keys()), y=list(top_genres.values()))\n",
    "plt.title('Top 10 Movie Genres')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_genres.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in ratings: 0\n",
      "Missing values in movies: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in ratings:\", ratings.isnull().sum().sum())\n",
    "print(\"Missing values in movies:\", movies.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze rating trends over time\n",
    "yearly_ratings = ratings.groupby('year')['rating'].mean().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='year', y='rating', data=yearly_ratings)\n",
    "plt.title('Average Rating by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.grid(True)\n",
    "plt.savefig('rating_trends.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Train-Test Split**\n",
    "The dataset is split into training and test sets:\n",
    "\n",
    "- 80% training, 20% testing\n",
    "- Stratified by userId to ensure each user has ratings in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. PREPARING TRAIN-TEST SPLIT...\n",
      "Training set: 19979624 ratings\n",
      "Test set: 4994907 ratings\n"
     ]
    }
   ],
   "source": [
    "# 3. Train-Test Split\n",
    "print(\"\\n3. PREPARING TRAIN-TEST SPLIT...\")\n",
    "\n",
    "# Stratify by userId to ensure each user has ratings in both training and test sets\n",
    "ratings_train, ratings_test = train_test_split(\n",
    "    ratings, test_size=0.2, random_state=42, \n",
    "    stratify=ratings['userId'].apply(lambda x: min(x % 10, 5))  # Simple stratification trick\n",
    ")\n",
    "print(f\"Training set: {len(ratings_train)} ratings\")\n",
    "print(f\"Test set: {len(ratings_test)} ratings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Mean-Centering**\n",
    "User ratings are mean-centered to improve recommendation quality:\n",
    "\n",
    "- Calculate mean rating for each user\n",
    "- Subtract user's mean from their ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. MEAN-CENTERING TRAINING RATINGS...\n"
     ]
    }
   ],
   "source": [
    "# 4. Mean-Center the Training Ratings\n",
    "user_mean_map = ratings_train.groupby('userId')['rating'].mean().to_dict()\n",
    "\n",
    "# Subtract each user's mean rating from their training ratings\n",
    "centered_data = []\n",
    "for row in ratings_train.itertuples(index=False):\n",
    "    uid, mid, rating = row.userId, row.movieId, row.rating\n",
    "    user_mean = user_mean_map[uid]\n",
    "    centered_rating = rating - user_mean\n",
    "    centered_data.append((uid, mid, centered_rating))\n",
    "\n",
    "centered_df = pd.DataFrame(centered_data, columns=['userId', 'movieId', 'centered_rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Sparse Matrix Creation**\n",
    "A sparse matrix is created from the centered ratings:\n",
    "\n",
    "- Maps user IDs and movie IDs to matrix indices\n",
    "- Creates a compressed sparse row matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. CREATING SPARSE MATRIX (MEAN-CENTERED) FOR TRAINING...\n",
      "Train sparse matrix shape: (162541, 41067)\n",
      "Non-zero entries: 19979624\n"
     ]
    }
   ],
   "source": [
    "# 5. Build Sparse Matrix from Centered Ratings\n",
    "print(\"\\n5. CREATING SPARSE MATRIX (MEAN-CENTERED) FOR TRAINING...\")\n",
    "user_ids = centered_df['userId'].unique()\n",
    "movie_ids = centered_df['movieId'].unique()\n",
    "\n",
    "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "movie_id_map = {mid: j for j, mid in enumerate(movie_ids)}\n",
    "\n",
    "# Reverse mappings (used later for recommendations)\n",
    "user_idx_map = {i: uid for uid, i in user_id_map.items()}\n",
    "movie_idx_map = {j: mid for mid, j in movie_id_map.items()}\n",
    "\n",
    "train_rows = centered_df['userId'].map(user_id_map).values\n",
    "train_cols = centered_df['movieId'].map(movie_id_map).values\n",
    "train_vals = centered_df['centered_rating'].values\n",
    "\n",
    "train_sparse = csr_matrix((train_vals, (train_rows, train_cols)),\n",
    "                          shape=(len(user_ids), len(movie_ids)))\n",
    "\n",
    "print(f\"Train sparse matrix shape: {train_sparse.shape}\")\n",
    "print(f\"Non-zero entries: {train_sparse.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. SVD Application**\n",
    "SVD is applied with different numbers of components to find the optimal model:\n",
    "\n",
    "- Tests multiple component counts: 10, 20, 50, 100\n",
    "- Evaluates each model using RMSE, MAE, and Precision@K\n",
    "- Visualizes predictions vs. actual ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. APPLYING SVD WITH USER-MEAN CENTERING AND OPTIMIZING COMPONENTS...\n",
      "\n",
      "Testing with 10 components...\n",
      "Explained variance: 0.0765\n",
      "Generating predictions on test set...\n",
      "[██████████████████████████████████████████████████] 100%\n",
      "RMSE with 10 components: 0.9021\n",
      "MAE with 10 components: 0.6896\n",
      "Precision@10 with 10 components: 0.7173\n",
      "\n",
      "Testing with 20 components...\n",
      "Explained variance: 0.1032\n",
      "Generating predictions on test set...\n",
      "[██████████████████████████████████████████████████] 100%\n",
      "RMSE with 20 components: 0.8969\n",
      "MAE with 20 components: 0.6850\n",
      "Precision@10 with 20 components: 0.7217\n",
      "\n",
      "Testing with 50 components...\n",
      "Explained variance: 0.1562\n",
      "Generating predictions on test set...\n",
      "[██████████████████████████████████████████████████] 100%\n",
      "RMSE with 50 components: 0.8936\n",
      "MAE with 50 components: 0.6824\n",
      "Precision@10 with 50 components: 0.7192\n",
      "\n",
      "Testing with 100 components...\n",
      "Explained variance: 0.2167\n",
      "Generating predictions on test set...\n",
      "[██████████████████████████████████████████████████] 100%\n",
      "RMSE with 100 components: 0.8958\n",
      "MAE with 100 components: 0.6847\n",
      "Precision@10 with 100 components: 0.7107\n"
     ]
    }
   ],
   "source": [
    "# 6. Apply SVD (Testing Multiple Components)\n",
    "print(\"\\n6. APPLYING SVD WITH USER-MEAN CENTERING AND OPTIMIZING COMPONENTS...\")\n",
    "n_components_list = [10, 20, 50, 100]  \n",
    "results = []\n",
    "\n",
    "for n_components in n_components_list:\n",
    "    print(f\"\\nTesting with {n_components} components...\")\n",
    "\n",
    "    # Fit SVD on the mean-centered training matrix\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    user_features = svd.fit_transform(train_sparse)\n",
    "    movie_features = svd.components_  # shape = (n_components, num_movies)\n",
    "\n",
    "    # Calculate explained variance\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(f\"Explained variance: {explained_variance:.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    test_users = ratings_test['userId'].values\n",
    "    test_movies = ratings_test['movieId'].values\n",
    "    test_ratings = ratings_test['rating'].values\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_tests = len(ratings_test)\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        if i % max(1, (total_tests // 10)) == 0:\n",
    "            progress_bar(i, total_tests)\n",
    "\n",
    "        uid = test_users[i]\n",
    "        mid = test_movies[i]\n",
    "        actual = test_ratings[i]\n",
    "\n",
    "        # If user/movie not in training, fall back to a global or user mean\n",
    "        if uid not in user_id_map or mid not in movie_id_map:\n",
    "            # Could use global average or fallback\n",
    "            pred_rating = 3.5\n",
    "        else:\n",
    "            u_idx = user_id_map[uid]\n",
    "            m_idx = movie_id_map[mid]\n",
    "\n",
    "            # Dot product in latent space\n",
    "            centered_pred = np.dot(user_features[u_idx], movie_features[:, m_idx])\n",
    "            # Add user mean back\n",
    "            user_mean = user_mean_map[uid]\n",
    "            pred_rating = user_mean + centered_pred\n",
    "\n",
    "        # Clip predictions\n",
    "        pred_rating = max(0.5, min(5.0, pred_rating))\n",
    "        predictions.append(pred_rating)\n",
    "        actuals.append(actual)\n",
    "\n",
    "    progress_bar(total_tests, total_tests)\n",
    "    print()\n",
    "\n",
    "    # Calculate error metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "    # Compute a simple Precision@K\n",
    "    k = 10\n",
    "    user_test_ratings = defaultdict(list)\n",
    "    for i in range(len(actuals)):\n",
    "        user_test_ratings[test_users[i]].append((predictions[i], actuals[i]))\n",
    "\n",
    "    precision_at_k = []\n",
    "    for user_id_, user_preds in user_test_ratings.items():\n",
    "        if len(user_preds) >= k:\n",
    "            user_preds.sort(key=lambda x: x[0], reverse=True)\n",
    "            # relevant if actual >= 4\n",
    "            relevant = sum(1 for _, act in user_preds[:k] if act >= 4.0)\n",
    "            precision_at_k.append(relevant / k)\n",
    "\n",
    "    avg_precision_at_k = np.mean(precision_at_k) if precision_at_k else 0\n",
    "\n",
    "    print(f\"RMSE with {n_components} components: {rmse:.4f}\")\n",
    "    print(f\"MAE with {n_components} components: {mae:.4f}\")\n",
    "    print(f\"Precision@{k} with {n_components} components: {avg_precision_at_k:.4f}\")\n",
    "\n",
    "    results.append((n_components, rmse, mae, avg_precision_at_k, explained_variance))\n",
    "\n",
    "    # Quick plots of predictions vs. actual\n",
    "    sample_size = min(1000, len(predictions))\n",
    "    sample_indices = np.random.choice(len(predictions), sample_size, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter([actuals[i] for i in sample_indices],\n",
    "                [predictions[i] for i in sample_indices],\n",
    "                alpha=0.3)\n",
    "    plt.plot([0.5, 5], [0.5, 5], 'r--')\n",
    "    plt.xlabel('Actual Ratings')\n",
    "    plt.ylabel('Predicted Ratings')\n",
    "    plt.title(f'Actual vs Predicted (User-Mean Centered) - {n_components} Components')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'prediction_scatter_{n_components}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Error distribution\n",
    "    errors = np.array(predictions) - np.array(actuals)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(errors, kde=True, bins=50)\n",
    "    plt.title(f'Error Distribution with {n_components} Components')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'error_distribution_{n_components}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Cleanup\n",
    "    del user_features, movie_features\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare results for different n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Compare results for different n_components\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "components, rmses, maes, precisions, variances = zip(*results)\n",
    "plt.plot(components, rmses, marker='o', label='RMSE')\n",
    "plt.plot(components, maes, marker='s', label='MAE')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Error Metric')\n",
    "plt.title('Error Metrics vs. #Components (Mean-Centered)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(components, precisions, marker='d', color='green')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel(f'Precision@{k}')\n",
    "plt.title(f'Precision@{k} vs. #Components')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(components, variances, marker='*', color='purple')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance vs. #Components')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# A simplified \"efficiency\" metric\n",
    "comp_efficiency = [c / time.time() for c in components]\n",
    "plt.plot(components, comp_efficiency, marker='x', color='orange')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Computational Efficiency')\n",
    "plt.title('Efficiency vs. #Components')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('component_analysis.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Model Selection**\n",
    "The best model is selected based on a combined score:\n",
    "\n",
    "- Normalized RMSE, precision, and variance metrics\n",
    "- Weighted combination of metrics\n",
    "- Rebuild final model with optimal component count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. SELECTING BEST MODEL...\n",
      "Best model uses 50 components:\n",
      "  - RMSE: 0.8936\n",
      "  - Precision@10: 0.7192\n",
      "  - Explained Variance: 0.1562\n"
     ]
    }
   ],
   "source": [
    "# 8. Select Best Model (Combined Score)\n",
    "print(\"\\n8. SELECTING BEST MODEL...\")\n",
    "\n",
    "min_rmse = min(rmses)\n",
    "max_rmse = max(rmses)\n",
    "normalized_rmses = [(max_rmse - r) / (max_rmse - min_rmse) if max_rmse > min_rmse else 0.5 for r in rmses]\n",
    "\n",
    "min_prec = min(precisions)\n",
    "max_prec = max(precisions)\n",
    "normalized_precs = [(p - min_prec) / (max_prec - min_prec) if max_prec > min_prec else 0.5 for p in precisions]\n",
    "\n",
    "min_var = min(variances)\n",
    "max_var = max(variances)\n",
    "normalized_vars = [(v - min_var) / (max_var - min_var) if max_var > min_var else 0.5 for v in variances]\n",
    "\n",
    "weights = (0.4, 0.3, 0.3)  # RMSE, precision, variance\n",
    "combined_scores = [weights[0]*nr + weights[1]*np_ + weights[2]*nv\n",
    "                   for nr, np_, nv in zip(normalized_rmses, normalized_precs, normalized_vars)]\n",
    "\n",
    "best_idx = combined_scores.index(max(combined_scores))\n",
    "best_n_components = components[best_idx]\n",
    "best_rmse = rmses[best_idx]\n",
    "best_precision = precisions[best_idx]\n",
    "\n",
    "print(f\"Best model uses {best_n_components} components:\")\n",
    "print(f\"  - RMSE: {best_rmse:.4f}\")\n",
    "print(f\"  - Precision@{k}: {best_precision:.4f}\")\n",
    "print(f\"  - Explained Variance: {variances[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Rebuild Final SVD Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. REBUILDING BEST MODEL FOR RECOMMENDATIONS...\n"
     ]
    }
   ],
   "source": [
    "# 9. Rebuild Final SVD Model\n",
    "print(\"\\n9. REBUILDING BEST MODEL FOR RECOMMENDATIONS...\")\n",
    "svd_final = TruncatedSVD(n_components=best_n_components, random_state=42)\n",
    "final_user_factors = svd_final.fit_transform(train_sparse)\n",
    "final_movie_factors = svd_final.components_\n",
    "\n",
    "# For interpretability\n",
    "final_explained_variance = svd_final.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(final_explained_variance)), final_explained_variance)\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by SVD Component (Best Model)')\n",
    "plt.savefig('explained_variance_best_model.png')\n",
    "plt.close()\n",
    "\n",
    "cumulative_variance = np.cumsum(final_explained_variance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(cumulative_variance)), cumulative_variance, marker='o')\n",
    "plt.axhline(y=0.9, color='r', linestyle='-', label='90% Variance')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance (Best Model)')\n",
    "plt.legend()\n",
    "plt.savefig('cumulative_variance_best_model.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Recommendation Utilities**\n",
    "Utility functions for generating recommendations:\n",
    "\n",
    "- ***predict_rating:*** Predicts rating for a user-movie pair\n",
    "- ***recommend_movies:*** Recommends top N movies for a user\n",
    "- ***find_similar_movies:*** Finds movies similar to a given movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Prediction & Recommendation Utilities\n",
    "def predict_rating(user_id, movie_id):\n",
    "    \"\"\"Predict rating for a single user–movie pair using mean-centering approach.\"\"\"\n",
    "    if user_id not in user_mean_map:\n",
    "        return 3.5  # fallback\n",
    "    \n",
    "    if user_id in user_id_map and movie_id in movie_id_map:\n",
    "        u_idx = user_id_map[user_id]\n",
    "        m_idx = movie_id_map[movie_id]\n",
    "        # Dot product in latent space\n",
    "        centered_pred = np.dot(final_user_factors[u_idx], final_movie_factors[:, m_idx])\n",
    "        # Add user mean back\n",
    "        user_mean = user_mean_map[user_id]\n",
    "        pred = user_mean + centered_pred\n",
    "    else:\n",
    "        # fallback\n",
    "        pred = 3.5\n",
    "    \n",
    "    # Clip to [0.5, 5.0]\n",
    "    return max(0.5, min(5.0, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, n_recommendations=5):\n",
    "    \"\"\"Recommend top N movies for a user based on final SVD model.\"\"\"\n",
    "    if user_id not in user_id_map:\n",
    "        print(f\"User {user_id} not found in training data\")\n",
    "        return []\n",
    "    \n",
    "    # Get all user-rated movies to exclude\n",
    "    user_rated = ratings_train[ratings_train['userId'] == user_id]['movieId'].unique()\n",
    "    \n",
    "    recs = []\n",
    "    for mid in movie_id_map:\n",
    "        if mid not in user_rated:\n",
    "            pr = predict_rating(user_id, mid)\n",
    "            recs.append((mid, pr))\n",
    "    recs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Build recommendation list with movie titles\n",
    "    recommendations = []\n",
    "    for mid, score in recs[:n_recommendations]:\n",
    "        row = movies[movies['movieId'] == mid]\n",
    "        if not row.empty:\n",
    "            title = row.iloc[0]['title']\n",
    "            genres = row.iloc[0]['genres']\n",
    "            recommendations.append((mid, title, genres, score))\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Safe cosine similarity.\"\"\"\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return np.dot(vec1, vec2) / (norm1 * norm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_movies(movie_id, n_similar=5):\n",
    "    \"\"\"Find movies similar to a given movie using final SVD model.\"\"\"\n",
    "    if movie_id not in movie_id_map:\n",
    "        print(f\"Movie {movie_id} not found in training data.\")\n",
    "        return []\n",
    "    \n",
    "    m_idx = movie_id_map[movie_id]\n",
    "    movie_vec = final_movie_factors[:, m_idx]\n",
    "    similarities = []\n",
    "    \n",
    "    for other_idx, other_id in movie_idx_map.items():\n",
    "        if other_id != movie_id:\n",
    "            other_vec = final_movie_factors[:, other_idx]\n",
    "            sim = safe_cosine_similarity(movie_vec, other_vec)\n",
    "            similarities.append((other_id, sim))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Build final list\n",
    "    similar_movies = []\n",
    "    for mid, sim in similarities[:n_similar]:\n",
    "        row = movies[movies['movieId'] == mid]\n",
    "        if not row.empty:\n",
    "            title = row.iloc[0]['title']\n",
    "            genres = row.iloc[0]['genres']\n",
    "            similar_movies.append((mid, title, genres, sim))\n",
    "    return similar_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11. Sample Recommendations**\n",
    "Generates sample recommendations for different types of users:\n",
    "\n",
    "- Light users (few ratings)\n",
    "- Medium users (moderate ratings)\n",
    "- Heavy users (many ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. GENERATING SAMPLE RECOMMENDATIONS...\n",
      "\n",
      "Recommendations for User 124860 (rated 19 movies, avg rating: 3.95):\n",
      "1. 2001: A Space Odyssey (1968) (Adventure|Drama|Sci-Fi) - Predicted Rating: 4.06\n",
      "2. Twelve Monkeys (a.k.a. 12 Monkeys) (1995) (Mystery|Sci-Fi|Thriller) - Predicted Rating: 4.04\n",
      "3. Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964) (Comedy|War) - Predicted Rating: 4.03\n",
      "4. Dark Knight, The (2008) (Action|Crime|Drama|IMAX) - Predicted Rating: 4.02\n",
      "5. Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001) (Comedy|Romance) - Predicted Rating: 4.02\n",
      "\n",
      "Recommendations for User 69862 (rated 99 movies, avg rating: 3.80):\n",
      "1. Shrek (2001) (Adventure|Animation|Children|Comedy|Fantasy|Romance) - Predicted Rating: 4.25\n",
      "2. Lord of the Rings: The Return of the King, The (2003) (Action|Adventure|Drama|Fantasy) - Predicted Rating: 4.22\n",
      "3. Lord of the Rings: The Two Towers, The (2002) (Adventure|Fantasy) - Predicted Rating: 4.21\n",
      "4. Pirates of the Caribbean: The Curse of the Black Pearl (2003) (Action|Adventure|Comedy|Fantasy) - Predicted Rating: 4.13\n",
      "5. Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981) (Action|Adventure) - Predicted Rating: 4.05\n",
      "\n",
      "Recommendations for User 72315 (rated 21314 movies, avg rating: 3.11):\n",
      "1. Maltese Falcon, The (1941) (Film-Noir|Mystery) - Predicted Rating: 5.00\n",
      "2. Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001) (Comedy|Romance) - Predicted Rating: 5.00\n",
      "3. North by Northwest (1959) (Action|Adventure|Mystery|Romance|Thriller) - Predicted Rating: 5.00\n",
      "4. Rear Window (1954) (Mystery|Thriller) - Predicted Rating: 5.00\n",
      "5. To Kill a Mockingbird (1962) (Drama) - Predicted Rating: 5.00\n"
     ]
    }
   ],
   "source": [
    "# 11. Sample Recommendations\n",
    "print(\"\\n10. GENERATING SAMPLE RECOMMENDATIONS...\")\n",
    "\n",
    "user_rating_counts = ratings_train['userId'].value_counts()\n",
    "light_user_id = user_rating_counts[user_rating_counts < 20].index[0] if any(user_rating_counts < 20) else user_rating_counts.index[0]\n",
    "medium_user_id = user_rating_counts[(user_rating_counts >= 20) & (user_rating_counts < 100)].index[0] if any((user_rating_counts >= 20) & (user_rating_counts < 100)) else user_rating_counts.index[1]\n",
    "heavy_user_id = user_rating_counts[user_rating_counts >= 100].index[0] if any(user_rating_counts >= 100) else user_rating_counts.index[2]\n",
    "\n",
    "sample_users = [light_user_id, medium_user_id, heavy_user_id]\n",
    "\n",
    "for uid in sample_users:\n",
    "    cnt = user_rating_counts[uid]\n",
    "    avg_r = user_mean_map.get(uid, 3.5)\n",
    "    print(f\"\\nRecommendations for User {uid} (rated {cnt} movies, avg rating: {avg_r:.2f}):\")\n",
    "    user_recs = recommend_movies(uid, n_recommendations=5)\n",
    "    for i, (mid, title, genres, score) in enumerate(user_recs, start=1):\n",
    "        print(f\"{i}. {title} ({genres}) - Predicted Rating: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12. Similar Movies**\n",
    "Finds and displays similar movies across popular genres:\n",
    "\n",
    "- Drama, Comedy, Action, Sci-Fi\n",
    "- Uses latent factors to compute similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. FINDING SIMILAR MOVIES IN POPULAR GENRES...\n",
      "\n",
      "Movies similar to 'Shawshank Redemption, The (1994)' (Crime|Drama):\n",
      "1. The Dancer and the Thief (2009) (Drama) - Similarity: 0.5362\n",
      "2. Flower Girl (2009) (Comedy|Romance) - Similarity: 0.4999\n",
      "3. A Single Rider (2017) (Drama|Mystery) - Similarity: 0.4977\n",
      "4. Grey Lady (2017) ((no genres listed)) - Similarity: 0.4645\n",
      "5. Pig (2011) (Drama|Mystery|Sci-Fi|Thriller) - Similarity: 0.4318\n",
      "\n",
      "Movies similar to 'Forrest Gump (1994)' (Comedy|Drama|Romance|War):\n",
      "1. A la mala (2015) (Comedy) - Similarity: 0.6567\n",
      "2. Life in the Doghouse (2018) (Documentary) - Similarity: 0.5674\n",
      "3. Under The Greenwood Tree (2005) (Drama|Romance) - Similarity: 0.5484\n",
      "4. The Double (1971) (Mystery|Thriller) - Similarity: 0.5154\n",
      "5. 3 Worlds of Gulliver, The (1960) (Adventure|Fantasy) - Similarity: 0.5065\n",
      "\n",
      "Movies similar to 'Matrix, The (1999)' (Action|Sci-Fi|Thriller):\n",
      "1. The End of Old Times (1990) (Comedy) - Similarity: 0.6879\n",
      "2. Perfect High (2015) ((no genres listed)) - Similarity: 0.5335\n",
      "3. I Could Go on Singing (1963) (Drama|Musical) - Similarity: 0.4968\n",
      "4. Katherine (1975) ((no genres listed)) - Similarity: 0.4948\n",
      "5. Aya of Yop City (2013) (Animation|Comedy) - Similarity: 0.4654\n",
      "\n",
      "Movies similar to 'Matrix, The (1999)' (Action|Sci-Fi|Thriller):\n",
      "1. The End of Old Times (1990) (Comedy) - Similarity: 0.6879\n",
      "2. Perfect High (2015) ((no genres listed)) - Similarity: 0.5335\n",
      "3. I Could Go on Singing (1963) (Drama|Musical) - Similarity: 0.4968\n",
      "4. Katherine (1975) ((no genres listed)) - Similarity: 0.4948\n",
      "5. Aya of Yop City (2013) (Animation|Comedy) - Similarity: 0.4654\n"
     ]
    }
   ],
   "source": [
    "# 12. Similar Movies\n",
    "print(\"\\n11. FINDING SIMILAR MOVIES IN POPULAR GENRES...\")\n",
    "\n",
    "genre_to_movie = {}\n",
    "for _, row in movies.iterrows():\n",
    "    for g in row['genres'].split('|'):\n",
    "        if g not in genre_to_movie:\n",
    "            genre_to_movie[g] = []\n",
    "        genre_to_movie[g].append(row['movieId'])\n",
    "\n",
    "popular_genre_movies = []\n",
    "for g in ['Drama', 'Comedy', 'Action', 'Sci-Fi']:\n",
    "    if g in genre_to_movie:\n",
    "        movie_ids_g = genre_to_movie[g]\n",
    "        # Find most rated in training set\n",
    "        ratings_per_movie = {m: len(ratings_train[ratings_train['movieId'] == m]) \n",
    "                             for m in movie_ids_g if m in ratings_train['movieId'].values}\n",
    "        if ratings_per_movie:\n",
    "            most_rated = max(ratings_per_movie.items(), key=lambda x: x[1])[0]\n",
    "            popular_genre_movies.append(most_rated)\n",
    "\n",
    "for mid in popular_genre_movies:\n",
    "    row = movies[movies['movieId'] == mid]\n",
    "    if not row.empty:\n",
    "        m_title = row.iloc[0]['title']\n",
    "        m_genres = row.iloc[0]['genres']\n",
    "        print(f\"\\nMovies similar to '{m_title}' ({m_genres}):\")\n",
    "        sims = find_similar_movies(mid, n_similar=5)\n",
    "        for i, (smid, stitle, sgenres, s) in enumerate(sims, start=1):\n",
    "            print(f\"{i}. {stitle} ({sgenres}) - Similarity: {s:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **13. Latent Space Visualization**\n",
    "Visualizes the latent space of movie factors:\n",
    "\n",
    "- Projects movie factors to 2D using PCA\n",
    "- Colors movies by decade\n",
    "- Shows relationships between movies in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12. VISUALIZING LATENT SPACE...\n"
     ]
    }
   ],
   "source": [
    "# 13. Optional: Visualize Latent Space\n",
    "print(\"\\n12. VISUALIZING LATENT SPACE...\")\n",
    "\n",
    "def extract_year(title):\n",
    "    try:\n",
    "        return int(title.strip()[-5:-1]) if title.strip()[-5:-1].isdigit() else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "movies['year'] = movies['title'].apply(extract_year)\n",
    "\n",
    "movies_with_year = movies.dropna(subset=['year'])\n",
    "decades = [(1950, 1959), (1960, 1969), (1970, 1979), (1980, 1989),\n",
    "           (1990, 1999), (2000, 2009), (2010, 2019)]\n",
    "\n",
    "sampled_decade_movies = []\n",
    "for start_year, end_year in decades:\n",
    "    dec_movies = movies_with_year[(movies_with_year['year'] >= start_year) & \n",
    "                                  (movies_with_year['year'] <= end_year)]\n",
    "    if len(dec_movies) > 0:\n",
    "        sampled_decade_movies.append(dec_movies.sample(min(10, len(dec_movies)), random_state=42))\n",
    "\n",
    "if len(sampled_decade_movies) > 0:\n",
    "    sampled_movies = pd.concat(sampled_decade_movies)\n",
    "    sampled_movie_ids = sampled_movies['movieId'].values\n",
    "\n",
    "    movie_latent_factors = []\n",
    "    movie_titles = []\n",
    "    decade_labels = []\n",
    "\n",
    "    for mid in sampled_movie_ids:\n",
    "        if mid in movie_id_map:\n",
    "            m_idx = movie_id_map[mid]\n",
    "            row = movies[movies['movieId'] == mid]\n",
    "            title = row.iloc[0]['title']\n",
    "            year = extract_year(title)\n",
    "            if year:\n",
    "                decade = f\"{year // 10 * 10}s\"\n",
    "            else:\n",
    "                decade = \"Unknown\"\n",
    "            movie_latent_factors.append(final_movie_factors[:, m_idx])\n",
    "            movie_titles.append(title)\n",
    "            decade_labels.append(decade)\n",
    "\n",
    "    if len(movie_latent_factors) > 0:\n",
    "        from sklearn.decomposition import PCA\n",
    "        arr = np.array(movie_latent_factors)\n",
    "        pca = PCA(n_components=2)\n",
    "        movie_factors_2d = pca.fit_transform(arr)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        decade_colors = {\n",
    "            '1950s': 'blue', '1960s': 'green', '1970s': 'red',\n",
    "            '1980s': 'purple', '1990s': 'orange', '2000s': 'brown', '2010s': 'black'\n",
    "        }\n",
    "\n",
    "        for i, (x, y) in enumerate(movie_factors_2d):\n",
    "            color = decade_colors.get(decade_labels[i], 'gray')\n",
    "            plt.scatter(x, y, color=color, alpha=0.7)\n",
    "            plt.annotate(movie_titles[i], (x, y), fontsize=8)\n",
    "\n",
    "        plt.title(\"Latent Space Visualization (Sampled Movies)\")\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"latent_space_visualization.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE! Total runtime: 2510.89 seconds.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nDONE! Total runtime: {elapsed:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: with all these implementation the total runtime is around 42min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
